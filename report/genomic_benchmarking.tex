\documentclass[9pt,twocolumn,twoside]{gsajnl}
% Use the documentclass option 'lineno' to view line numbers

\usepackage{epstopdf}

\articletype{inv} % article type
% {inv} Investigation
% {gs} Genomic Selection
% {goi} Genetics of Immunity
% {gos} Genetics of Sex
% {mp} Multiparentl Populations

\runningtitle{Deep Learning Benchmarks for Genomic Sequences}% For use in the footer
\runningauthor{Jordan Winemiller \textit{2026}}

\title{Deep Learning in Genomic Sequence Benchmarking: Implementing CNNs with a
Focus in Reproducibility}

\author[1,$\dagger$]{Jordan Winemiller}

\affil[1]{Department of Electrical Engineering and Computer Science, Florida
Atlantic Universtiy, Boca Raton, Florida, USA;}

% Use the \equalcontrib command to mark authors with equal
% contributions, using the relevant superscript numbers
%\equalcontrib{1}
%\equalcontrib{2}

\begin{abstract}
As deep learning becomes increasingly prevalent in genomics, maintaining
reproducibility in model development and evaluation is essential. In the
biosciences, particularly for large genomic sequence datasets, benchmarking
has become a valuable tool for enabling repeatable sequence classification
experiments. This project uses publicly available genomic sequence benchmark
datasets to implement convolutional neural networks \textit{(CNNs)} for
regulatory element classification and to examine the practical challenges of
reproducing published workflows. We reimplement baseline CNN models using
contemporary deep learning libraries, document the full training and evaluation
pipeline, and compare our results with those reported in prior work. In doing
so, we highlight common issues arising from unmaintained code bases, including
dependency incompatibilities and undocumented preprocessing steps, and assess
their impact on downstream performance and repeatability. The outcome is a
transparent, reproducible set of scripts and guidelines intended to help
students and researchers reliably apply deep learning to benchmark genomic
sequence datasets.
\end{abstract}

\keywords{Deep Learning; Convolutional Neural Networks; Reproducibility;
Benchmarking; Computational Genomic}

\dates{\rec{02 06, 2026} \acc{02 06, 2026}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
%\slugnote
%\firstpagefootnote
\vspace{-13pt}% Only used for adjusting extra space in the left column of the first page

\section{Introduction}
\label{sec:intro}
\lettrine[lines=2]{\color{color2}G}{enomic} sequence classification has become a
central task in computational biology, enabling researchers to characterize
functional elements and regulatory patterns directly from DNA. Building on this
momentum, the work presented here focuses on recreating previously proposed
benchmarks from Genomics Benchmarks: A Collection of Datasets for Genomic
Sequence Classification \citep{gb}, which was designed to lower the barrier for
experimenting with genomic data. These benchmarks provide a structured setting
for evaluating models while promoting transparency and comparability across
studies.

This work is intended for students and researchers in computational genomics,
biostatistics, and computer science who seek practical, reproducible examples of
applying deep learning to standardized genomic sequence benchmarks. 

\subsection{Background}
\label{sec:intro:bg}
Motivated by a background in biostatistics and computer science, this project
aims to deepen practical experience with genomic datasets and to lay the
foundation for future collaborations with faculty specializing in genomics.
Recent literature has demonstrated that convolutional neural networks and
related deep learning architectures can effectively learn predictive
representations of genomic sequences, capturing both local motifs and
higher-order patterns. In this context, the present work implements deep
learning models on the established benchmark datasets and examines the practical
challenges of achieving robust, repeatable results. By emphasizing
reproducibility and clear experimental design, this project seeks to provide a
useful reference for researchers and students who are beginning to work with
deep learning methods in genomics.
Implementation details and examples are available on \textit{GitHub \citep{
    ghbg}}
and \textit{
\href{https://colab.research.google.com/drive/1bx2C30r8Bm4BuFnv6Ugh0vlni72YpwoI}
{Google Colab}}.

\section{Materials and Methods}
\label{sec:mm}

\subsection{Datasets}
\label{sec:mm:data}
Genomic datasets are highly complex due to diverse data formats, pervasive
missing values, variable sequence lengths, extreme scale, class imbalance in
functional annotations, and batch effects from different sequencing technologies
or experimental conditions. The data formats available in the materials 
are stored as genomic coordinates. Some sequences were previously mapped using
the \textit{seq2loc} Python package that specializes in mapping sequences for
fasta files, human enhancers, and non-TATA promoters to genomic coordinates
Browser Extensible Data \textit{(BED)} format.

The selected datasets in table \ref{tbl:datasets} for benchmarking consist 
of two (2) of non-uniform sequence and two (2) of uniform sequence length, 
with one containing unbalanced classes. The first non-uniform set 
\textit{dummy\_mouse\_enhancers\_ensembl} was designed as a small 'dummy' 
dataset for prototyping. The second non-uniform set
\textit{drosophila\_enhancers\_stark} consisting of the drosophila enhancer
\citep{kvon}. Both uniform datasets were previously converted using 
\textit{seq2loc} to the BED format. The first set 
\textit{human\_enhancers\_cohn} consists of human enhancers \citep{cohn}. 
The second set \textit{human\_nontata\_promoters} of non-TATA promoters 
\citep{Umarov} has a class imbalance.

The materials include additional datasets that are  available through the 
\textit{genomic\_benchmarks} \citep{ghgb} Python package by \textbf{ML-Bioinfo-CEITEC}
, and \href{https://huggingface.co/}{Hugging Face}.

\begin{table*}[thp]
\centering
\caption{Description of selected datasets in genomic-benchmarks package}
\begin{tableminipage}{\textwidth}
\begin{tabularx}{\textwidth}{p{5cm}XXXXXX}
    \hline
    \bf{Name} & \bf{Number \newline of Sequences} & \bf{Number \newline of Classes} 
    & \bf{Class \newline Ratio}\footnote{Ratio of the largest class} & 
    \bf{Median \newline Length}\footnote{Median length for all sequences in the dataset}
    & \bf{Sequence \newline Length Range}\footnote{Minimum and maximum length for all
    sequences in the dataset. *Note: if blank all sequence are the length} \\
    \hline
    dummy\_mouse\_enhancers\_ensembl & 1210 & 2 & 1.0 & 2381 & 331 to 4476 \\
    drosophila\_enhancers\_stark & 6914 & 2 & 1.0 & 2142 & 236 to 3237 \\
    human\_enhancers\_cohn & 27791 & 2 & 1.0 & 500 & - \\
    human\_nontata\_promoters & 36131 & 2 & 1.2 & 251 & - \\
    \hline
\end{tabularx}
\label{tbl:datasets}
\end{tableminipage}
\end{table*}

\subsection{Statistical Methods}
\label{sec:mm:sm}
The classification metrics for predictions will look at \textit{True Positive,
True Negative, False Positive, False Negative, Precision,} and \textit{Recall}
\textit{(eq. \ref{eq:bg})}. These metrics will be used to measure the 
$Accuracy$ \textit{(eq. \ref{eq:acc})} and $F_1\ Score$ 
\textit{(eq. \ref{eq:f1})} for benchmarking the models. The $F_1\ Score$
metric will be used for binary predictions. Otherwise, the $F_\beta\ Score$
\textit{(eq. \ref{eq:fbeta})} metric will be used for any multi-class 
classification. In addition to these metrics, $Cost\ Functions$ will be 
monitored during model training.

\subsubsection{Calculations}
\label{sec:mm:sm:c}
\begin{equation}
\begin{split}
TP = True\ Positive \\
TN = True\ Negative \\
FP = False\ Positive \\
FN = False\ Negative \\
Precision = \frac{TP}{TP + FP} \\
Recall = \frac{TN}{TN + FP} \\
\end{split}
\label{eq:bg}
\end{equation}

\begin{equation}
Accuracy = \frac{FP}{TP + FP} = 1 - \frac{TP}{TP + FP}
\label{eq:acc}
\end{equation}

\begin{equation}
F_1\ Score = 2 * \frac{precision * recall}{precision + recall}
\label{eq:f1}
\end{equation}

\begin{equation}
F_\beta\ Score = \frac{(1+\beta^2) * TP}{(TP + FN) * \beta^2 + (TP + FP)}
\label{eq:fbeta}    
\end{equation}

\subsection{Model Architectures}
\label{sec:mm:ma}
Convolutional neural networks (CNNs) are powerful deep learning architectures
that extract biologically meaningful features from genomic sequences with
minimal hand-engineered input. These models excel at DNA feature extraction by
automatically discovering regulatory elements such as the spacing patterns
that distinguish promoters from enhancers, and demonstrate robustness
across diverse species and sequence contexts \citep{Zhang}. The output of the
CNN layer is fed to the Max pooling layer, which helps to reduce the
spatial dimensions of the feature maps while retaining the most essential
information by selecting the maximum value within each pooling window. This
allows the network to focus on the most significant features and become less
sensitive to small variations in the input sequence, making the model more
robust. To normalize the inputs to the layers, a batch normalization layer
is introduced before the next layer \citep{dna}.

The initial setup for this experiment is to implement two (2) deep convolutional 
neural networks with the \textbf{Keras} package. The first network will consist
of three (3) convolution units with a \textbf{Convolutional 1D Layer} for one (1)
dimensional data, \textbf{Batch Normalization Layer}, and a 
\textbf{Max Pooling 1D Layer} with a max pooling size of two (2). The fully connected 
layer will consist of a \textbf{Dropout Layer} with \textbf{30\%} of the input units
to be removed, a \textbf{Global Average Pooling 1D Layer} and a \textbf{Dense Layer}. 
The three (3) convolutional layers will all have a \textbf{kernel size} of 8, 
\textbf{filters} of 32, 16, and 4 in descending order, and implements the 
\textit{Rectified Linear Units (ReLU)} \textit{(eq. \ref{eq:relu})} activation function. 
\textit{Figure \ref{fig:first_cnn}} shows the details of the first CNN architecture.

\begin{figure}[hp]
\centering
\includegraphics[height=12cm]{assets/figures/basic_cnn_architure}
\caption{First Model Architecture}
\label{fig:first_cnn}
\end{figure}

The second network will consist of three (3) convolution units with a 
\textbf{Convolutional 1D Layer} for the one (1) dimensional data, 
\textbf{Batch Normalization Layer}, and a \textbf{Max Pooling 1D Layer} with a max 
pooling size of two (2) as the first CNN architecture. The fully connected layer will 
consist of a \textbf{Flatten Layer} and two (2) \textbf{Dense Layers}. The three 
(3) convolutional layers will all have a \textbf{kernel size} of 8, \textbf{filters}
of 16, 8, and 4 in descending order, and use the \textit{ReLU} activation function. 
The first of the dense layers will have 512 units and use the \textit{ReLU} activation
function. \textit{Figure \ref{fig:second_cnn}} shows the details of the second CNN
architecture.

\begin{figure}[hp]
\centering
\includegraphics[height=12cm]{assets/figures/second_cnn_architure}
\caption{Second Model Architecture}
\label{fig:second_cnn}
\end{figure}

The last dense layer in both networks will have one (1) unit for binary classification 
using the \textit{Sigmoid} \textit{(eq. \ref{eq:sigmoid})} activation function, and 
the number of classes for the units for multi-class classification using the 
\textit{Softmax} \textit{(eq. \ref{eq:softmax})} activation function.

\subsubsection{Activation Functions}
\label{sec:mm:ma:af}
\begin{equation}    
ReLU(x) = x^+ = max(0, x)= \frac{x + |x|}{2} = 
\begin{cases}
    x\ if\ x>0, \\
    0\ x \leq0 
\label{eq:relu}
\end{cases}
\end{equation}

\begin{equation}
Sigmoid = \sigma(x) = \frac{1}{1 + e^{-x}}
\label{eq:sigmoid}
\end{equation}

\begin{equation}
Softmax = \sigma(z)_i = \frac{e^{z_i}}{\Sigma_{j=1}^{K}e^{z_j}}
\label{eq:softmax}
\end{equation}

\subsection{Implementation}
\label{sec:impl}
The \textbf{genomic-benchmarks} Python package from \textbf{ML-Bioinfo-CEITEC} is a 
GitHub repository\citep{ghgb} that collects benchmarks for genomic
sequence classification. The repository includes the datasets, source code, 
helper functions, and examples. The repository is also available via \textit{PIP}
installation. The original implementation of the datasets and example code do not 
use a validation set. \textit{Figure \ref{fig:dummy}} shows the initial information
prototype dataset. \textit{Figure \ref{fig:dataset}} shows the type of the 
training dataset. 

\begin{figure}[hp]
\centering
\includegraphics[width=0.4\textwidth]{assets/figures/dummy_data}
\caption{Initial Prototype Dataset}
\label{fig:dummy}
\end{figure}

\begin{figure}[hp]
\centering
\includegraphics[width=0.4\textwidth]{assets/figures/dataset_type}
\caption{Dataset Type}
\label{fig:dataset}
\end{figure}

The list of datasets available from \textit{genomic-benchmarks} Python package
can be found using \textit{code \ref{cd:datasets}}.

\lstinputlisting[
    language=Python,
    % float=,
    caption={List the Available Datasets},
    label={cd:datasets},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/datasets.py}

There are other dependencies that are needed to use the helper functions and
models (This is covered in the next section on \textit{Complications}). The 
datasets are available as built-in data classes for \textit{TensorFlow} and 
\textit{Torch}. The text files were loaded and processed in a batch size of 64
into a TensorFlow PreFetchDataSet. The network architecture figures utilized
the python package \textit{mermaid-py}. The \textit{code \ref{cd:pip}} shows the pip 
installation, and \textit{code \ref{cd:imports}} shows the imports and setup.  

\lstinputlisting[
    language=Python,
    % float=,
    caption={PIP installation},
    label={cd:pip},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/pip_install.py}

\lstinputlisting[
    language=Python,
    % float=,
    caption={Setup and Imports},
    label={cd:imports},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/setup_and_imports.py}

Downloading the datasets from the \textit{genomic-benchmarks} Python 
package creates a hidden folder in your working directory with the 
setup in \textit{code \ref{cd:ds_download}} will only download a 
non-existent dataset. The info command will produce information 
about the dataset from the \textit{metadata.yaml} file that 
accompanies each dataset.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Dataset Download},
    label={cd:ds_download},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/download_datasets.py}

The dataset needed to be transformed using a \textit{one-hot} encoding for
each class. This process vectorized the text. The vectorization process 
\textit{codes \ref{cd:vec_layer}} and \textit{\ref{cd:vec_text}} show the 
methods used to facilitate the data transformation.  

\lstinputlisting[
    language=Python,
    % float=,
    caption={Vectorization Layer},
    label={cd:vec_layer},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/vectorize_layer.py}

\newline \break
\lstinputlisting[
    language=Python,
    % float=,
    caption={Vectorize Text},
    label={cd:vec_text},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/vectorize_text.py}

Providing transparent evidence of the model's performance is the cornerstone
of benchmarking. \textit{Code \ref{cd:plots}} describes the implementation 
for capturing these important metrics and providing visual evidence of the
CNN model for training and evaluation and new unseen test data. The graphs
produced will contain the accuracy, cost functions 
\textit{(i.e., learning curve)}, and $F_1\ score$ for training. The 
method also provides metrics for the models evaluation of the testing
dataset.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Displaying Plots and Metrics},
    label={cd:plots},
    basicstyle=\scriptsize,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/display_metrics.py}

\subsection{Complications}
\label{sec:comp}
There were complications due to when \textit{genomic-benchmarks} Python package
was design and built. The authors built the source code with the helper functions
to work with up to \textit{python3.10}. There are a few dependencies in
the source code that could not be used. Including \textit{tensorflow-addons} for 
$F_1\ score$ due to the \textit{Keras} package version (which is addressed in the 
next section), \textit{torchtext} for tokenization methods in the source code,
which does not have a compatible \textit{torch} version that can be installed in
\textit{Google Colab}.

There were other complications due to the data being not 
uniform in length, adding complexity of using native \textit{numpy} 
arrays. There was another issue with how the \textit{one-hot} encoding source 
code was working and being utilized in the model (which is addressed in the next
section). \textit{Figures \ref{fig:tensorflow}, \ref{fig:torch}, \ref{fig:numpy}}
show the complications.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{assets/figures/tensorflow_issue}
\caption{Tensorflow-addons Package Issue}
\label{fig:tensorflow}
\end{figure}

\begin{figure}[hp]
\centering
\includegraphics[width=0.4\textwidth]{assets/figures/torch_reinstall_error}
\caption{Torchtext Package Issue}
\label{fig:torch}
\end{figure}

\begin{figure}[hp]
\centering
\includegraphics[width=0.4\textwidth]{assets/figures/numpy_issue}
\caption{Numpy Data Shape Issue}
\label{fig:numpy}
\end{figure}

\subsection{Mitigation Attempts}
\label{sec:mig}
\textbf{Attempted:} Working through the source code and complications, I decided to
lower the python version to the lowest available in the google Colab 
environment. Initially, there was an attempt to switch the version shown in 
\textit{figure \ref{fig:env}}.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{assets/figures/python_version}
\caption{Python3.11 Installation}
\label{fig:env}
\end{figure}

This was abandoned due to time constraints. After this change was made, 
the first model was able to produce results with minimal additional workarounds.   

\textbf{Fixed:} The second model required additional new layers to be added to 
the architecture to handle the output dimension from the last 
\textbf{Convolutional Layer} to the \textbf{Flatten Layer}. 
A \textbf{Global Average Pooling Layer} was added to the CNN architecture. 
\textit{Figure \ref{fig:second_cnn_update}} shows the updated architecture for
the second model.

\begin{figure}[H]
\centering
\includegraphics[height=18cm]{assets/figures/second_cnn_architure_update}
\caption{Second Model Architecture Update}
\label{fig:second_cnn_update}
\end{figure}

\textbf{Fixed:} The $F_1\ Score$ that is implemented in the 
\textit{genomic-benchmarks} Python package requires a version of \textit{Keras}
that is not supported in the available version of \textit{Google Colab}. A Python
function \textit{code \ref{cd:f1}} was written for the implementation of the 
$F_1\ Score$ metric for the evaluation of the model.

\lstinputlisting[
    language=Python,
    % float=,
    caption={$F_1\ Score$ Calculation},
    label={cd:f1},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/f1_score.py}

\section{Results}
\label{sec:result}

\textbf{Utility Code}

To facilitate the execution for training and testing the models 
on multiple datasets; utility code was built to allow for a
repeatable setup. \textit{Code \ref{cd:update}} allows for 
an update for training and test datasets in the notebook. This
was required because the variables existing in a global scope.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Update the Dataset Selection.},
    label={cd:update},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/update_train_test_sets.py}

The models were trained and evaluated using the method
\textit{code \ref{cd:te}} to populate a dictionary with model
metrics after the models were compiled, trained and evaluated.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Train and Evaluate Models.},
    label={cd:te},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/training_model.py}

Repeating the setup with the above utilities for training and 
evaluation of the models on each dataset was reduced to the
\textit{code \ref{cd:runner}}.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Example Training and Evaluation Run.},
    label={cd:runner},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/example_model_run.py}

\subsection{Models}
\label{sec:result:mod}
The models for these experiments were built with the
\textit{Keras} Python package using the architectures mentioned 
in the model architecture \ref{sec:mm:ma}. All of the experiments
ran for 10 epochs. The baseline model was built with 
\textit{code \ref{cd:m1}}, and the final model was compiled
with the \textit{code \ref{cd:m2}}.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Baseline Model.},
    label={cd:m1},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/initial_model.py}

\lstinputlisting[
    language=Python,
    % float=,
    caption={Final Model.},
    label={cd:m2},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/final_model.py}

\subsection{Dataset: Dummy Mouse Enhancers Ensembl}
\label{sec:result:r1}
The models were trained on 968 sequences for 10 epochs. Then,
tested on 242 sequences. The results of the training are shown 
in \textit{figure \ref{fig:m0}}. The 
\textbf{total loss, accuracy, and $F_1$ score} evaluation metrics
are shown in table \ref{tbl:m0}. 
\textit{Note: this is dataset index 0}.

\begin{figure}[H]
\centering
\includegraphics[height=16cm]{assets/model_runs/model_0.png}
\caption{Model Training for dummy\_mouse\_enhancers\_ensembl}
\label{fig:m0}
\end{figure}

\begin{table}[H]
\caption{Evaluation Metrics for dummy\_mouse\_enhancers\_ensembl Dataset}
\begin{tableminipage}{0.45\textwidth}
\begin{tabularx}{\textwidth}{XXXX}
    \hline
    \bf{Model Name} & \bf{Total Loss} & \bf{Accuracy} & \bf{$F_1$ Score} \\
    \hline
    \bf{Basic} & 0.5797 & 66.82\% & 33.13 \\
    \bf{Final} & 0.9998 & 50.64\% & 0.76 \\
    \hline
\end{tabularx}
\label{tbl:m0}
\end{tableminipage}
\end{table}

\subsection{Dataset: Drosophila Enhancers Stark}
\label{sec:result:r2}
The models were trained on 5,184 sequences for 10 epochs. Then,
tested on 1,730 sequences. The results of the training are shown 
in \textit{figure \ref{fig:m1}}. The 
\textbf{total loss, accuracy, and $F_1$ score} evaluation metrics
are shown in table \ref{tbl:m1}. 
\textit{Note: this is dataset index 1}.

\begin{figure}[H]
\centering
\includegraphics[height=16cm]{assets/model_runs/model_1.png}
\caption{Model Training for drosophila\_enhancers\_stark Dataset}
\label{fig:m1}
\end{figure}

\begin{table}[H]
\caption{Evaluation Metrics for drosophila\_enhancers\_stark}
\begin{tableminipage}{0.45\textwidth}
\begin{tabularx}{\textwidth}{XXXX}
    \hline
    \bf{Model Name} & \bf{Total Loss} & \bf{Accuracy} & \bf{$F_1$ Score} \\
    \hline
    \bf{Basic} & 1.3692 & 49.94\% & 40.93 \\
    \bf{Final} & 0.8922 & 50.46\% & 0.82 \\
    \hline
\end{tabularx}
\label{tbl:m1}
\end{tableminipage}
\end{table}

\subsection{Dataset: Human Enhancers Cohn}
\label{sec:result:r3}
The models were trained on 20,843 sequences for 10 epochs. Then,
tested on 6,948 sequences. The results of the training are shown 
in \textit{figure \ref{fig:m2}}. The 
\textbf{total loss, accuracy, and $F_1$ score} evaluation metrics
are shown in table \ref{tbl:m2}. 
\textit{Note: this is dataset index 2}.

\begin{figure}[H]
\centering
\includegraphics[height=16cm]{assets/model_runs/model_2.png}
\caption{Model Training for human\_enhancers\_cohn Dataset}
\label{fig:m2}
\end{figure}

\begin{table}[H]
\caption{Evaluation Metrics for human\_enhancers\_cohn}
\begin{tableminipage}{0.45\textwidth}
\begin{tabularx}{\textwidth}{XXXX}
    \hline
    \bf{Model Name} & \bf{Total Loss} & \bf{Accuracy} & \bf{$F_1$ Score} \\
    \hline
    \bf{Basic} & 0.7929 & 52.60\% & 40.45 \\
    \bf{Final} & 0.6263 & 65.09\% & 14.97 \\
    \hline
\end{tabularx}
\label{tbl:m2}
\end{tableminipage}
\end{table}

\subsection{Dataset: Human Non-TATA Promoters}
\label{sec:result:r4}
The models were trained on 27,097 sequences for 10 epochs. Then,
tested on 9,034 sequences. The results of the training are shown 
in \textit{figure \ref{fig:m3}}. The 
\textbf{total loss, accuracy, and $F_1$ score} evaluation metrics
are shown in table \ref{tbl:m3}. 
\textit{Note: this is dataset index 3}.

\begin{figure}[H]
\centering
\includegraphics[height=16cm]{assets/model_runs/model_3.png}
\caption{Model Training for human\_nontata\_promoters Dataset}
\label{fig:m3}
\end{figure}

\begin{table}[H]
\caption{Evaluation Metrics for human\_nontata\_promoters}
\begin{tableminipage}{0.45\textwidth}
\begin{tabularx}{\textwidth}{XXXX}
    \hline
    \bf{Model Name} & \bf{Total Loss} & \bf{Accuracy} & \bf{$F_1$ Score} \\
    \hline
    \bf{Basic} & 0.5817 & 71.56\% & 23.30 \\
    \bf{Final} & 0.6068 & 66.36\% & 20.33 \\
    \hline
\end{tabularx}
\label{tbl:m3}
\end{tableminipage}
\end{table}

\subsection{Benchmark Analysis}
\label{sec:result:an}
In benchmarking the models on the selected datasets, the models showed
difficulties in obtaining any significant training loss or accuracy. 
Upon evaluation of the models, the accuracy appears to predict classes
at random expected with the \textit{human non-TATA promoters}, but that
dataset had imbalance favoring the predicted class. The $F_1$ scores
for evaluation performed better on the baseline model, which could be 
due in part to the additional \textit{global average pooling} layer 
that had to be added to the model architecture. The original final model
architecture was compiled using \textit{Pytorch}, but that should 
not account for the amount of lack of performance.

\section{Improvements}
There are a few improvements beyond the updates required for newer versions
of the packages used in the experiments. Future experiments would benefit 
from the addition of validation sets to test for overfitting during training.
Moving the activation layer for the CNN units to after the batch normalization
layer. Increasing the number of epochs for training iterations. Using built-in
optimizers to allow for control of the learning rates and built-in data 
encoding for input layers to understand the number of parameters in the
models.

Other improvements consists of testing different model architectures, 
using data augmentation, and handling of sparse data.

\section{Conclusions}
Although initial attempts to reproduce the previous results with the
deep learning models did not produce the expected results, mainly due to
the implementation obstacles. The undertaking of a project to become 
familiar with genomic data and developing Convolutional Neural Networks 
through updating and working through the proposed architectures;
there is an excitement to continue exploring more deep learning 
opportunities in the field of genomics.

\section{Miscellaneous Code}
\textbf{Additional code used to create the report:}

The diagrams use \textit{mermaid-py} and 
\textit{code \ref{cd:mermaid}} shows an example use case for
this project.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Mermaid Diagram Example},
    label={cd:mermaid},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/mermaid_example.py}

Verification of the length of the dataset during exploratory data analysis
and data cleansing is critical in work with genomic data. 
\textit{Code \ref{cd:check_len}} shows the implementation of this method.

\lstinputlisting[
    language=Python,
    % float=,
    caption={Checking the Length Method},
    label={cd:check_len},
    basicstyle=\tiny,
    numbers=none,
    showstringspaces=false,
]{assets/code_examples/check_lengths.py}

Also, the package \textit{listings} package was added and renamed to 
display \textbf{Code} instead of \textbf{Listing} using the 
\textit{renewcommand} in the class file.

\section{Data availability}
All datasets are available for GitHub \citep{ghgb}, and all code covered in this 
article can be found at \citep{gh} or at \href{https://colab.research.google.com/drive/1bx2C30r8Bm4BuFnv6Ugh0vlni72YpwoI}
{Google Colab}.

\section{Acknowledgments}
Not applicable.

\section{Conflicts of interest}
There are no conflicts of interest.

\bibliography{bibliography}

\end{document} 